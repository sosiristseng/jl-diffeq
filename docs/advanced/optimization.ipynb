{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization Problems\n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Optimization_problem\n",
    "\n",
    "Given a target (loss) function and a set of parameters. Find the parameters set (subjects to constraints) to minimize the function.\n",
    "\n",
    "- Curve fitting: [LsqFit.jl](https://github.com/JuliaNLSolvers/LsqFit.jl)\n",
    "- General optimization problems: [Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl)\n",
    "- Use with ModelingToolkit: [Optimization.jl](https://github.com/SciML/Optimization.jl)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Curve fitting using LsqFit\n",
    "\n",
    "[LsqFit.jl](https://github.com/JuliaNLSolvers/LsqFit.jl) package is a small library that provides basic least-squares fitting in pure Julia."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LsqFit\n",
    "@. model(x, p) = p[1] * exp(-x * p[2])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xdata = range(0, stop=10, length=20)\n",
    "ydata = model(xdata, [1.0 2.0]) + 0.01 * randn(length(xdata))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial guess"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p0 = [0.5, 0.5]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit the model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fit = curve_fit(model, xdata, ydata, p0; autodiff=:forwarddiff)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameters"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "coef(fit)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Curve fitting using Optimization.jl"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Optimization\n",
    "using OptimizationOptimJL\n",
    "\n",
    "@. model(x, p) = p[1] * exp(-x * p[2])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate data"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xdata = range(0, stop=10, length=20)\n",
    "ydata = model(xdata, [1.0 2.0]) + 0.01 * randn(length(xdata))\n",
    "\n",
    "function lossl2(p, data)\n",
    "    x, y = data\n",
    "    y_pred = model(x, p)\n",
    "    return sum(abs2, y_pred .- y)\n",
    "end\n",
    "\n",
    "p0 = [0.5, 0.5]\n",
    "data = [xdata, ydata]\n",
    "prob = OptimizationProblem(lossl2, p0, data)\n",
    "res = solve(prob, Optim.NelderMead())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2D Rosenbrock Function\n",
    "\n",
    "From: https://docs.sciml.ai/ModelingToolkit/stable/tutorials/optimization/\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "\n",
    "Find $(x, y)$ that minimizes the loss function $(a - x)^2 + b(y - x^2)^2$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ModelingToolkit\n",
    "using Optimization\n",
    "using OptimizationOptimJL"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variables begin\n",
    "    x, [bounds = (-2.0, 2.0)]\n",
    "    y, [bounds = (-1.0, 3.0)]\n",
    "end\n",
    "\n",
    "@parameters a b"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the target (loss) function\n",
    "The optimization algorithm will try to minimize its value"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss = (a - x)^2 + b * (y - x^2)^2"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the OptimizationSystem"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@mtkbuild sys = OptimizationSystem(loss, [x, y], [a, b])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initial guess"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "u0 = [x => 1.0, y => 2.0]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "parameters"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p = [a => 1.0, b => 100.0]"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "ModelingToolkit can generate gradient and Hessian to solve the problem more efficiently."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prob = OptimizationProblem(sys, u0, p, grad=true, hess=true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Solve the problem\n",
    "The true solution is (1.0, 1.0)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "u_opt = solve(prob, GradientDescent())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding constraints\n",
    "`OptimizationSystem(..., constraints = cons)`"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@variables begin\n",
    "    x, [bounds = (-2.0, 2.0)]\n",
    "    y, [bounds = (-1.0, 3.0)]\n",
    "end\n",
    "\n",
    "@parameters a = 1 b = 100\n",
    "\n",
    "loss = (a - x)^2 + b * (y - x^2)^2"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constraints are define using `≲` (`\\lesssim`) or `≳` (`\\gtrsim`)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cons = [\n",
    "    x^2 + y^2 ≲ 1,\n",
    "]\n",
    "@mtkbuild sys = OptimizationSystem(loss, [x, y], [a, b], constraints=cons)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "u0 = [x => 0.14, y => 0.14]\n",
    "prob = OptimizationProblem(sys, u0, grad=true, hess=true, cons_j=true, cons_h=true)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use interior point Newton method for constrained optimization"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "solve(prob, IPNewton())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameter estimation\n",
    "\n",
    "From: https://docs.sciml.ai/DiffEqParamEstim/stable/getting_started/\n",
    "\n",
    "`DiffEqParamEstim.jl` is not installed with `DifferentialEquations.jl`. You need to install it manually:\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"DiffEqParamEstim\")\n",
    "using DiffEqParamEstim\n",
    "```\n",
    "\n",
    "The key function is `DiffEqParamEstim.build_loss_objective()`, which builds a loss (objective) function for the problem against the data. Then we can use optimization packages to solve the problem.\n",
    "\n",
    "### Estimate a single parameter from the data and the ODE model\n",
    "\n",
    "Let's optimize the parameters of the Lotka-Volterra equation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using OrdinaryDiffEq\n",
    "using Plots\n",
    "using DiffEqParamEstim\n",
    "using ForwardDiff\n",
    "using Optimization\n",
    "using OptimizationOptimJL"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example model"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function lotka_volterra!(du, u, p, t)\n",
    "    du[1] = dx = p[1] * u[1] - u[1] * u[2]\n",
    "    du[2] = dy = -3 * u[2] + u[1] * u[2]\n",
    "end\n",
    "\n",
    "u0 = [1.0; 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5] ## The true parameter value\n",
    "prob = ODEProblem(lotka_volterra!, u0, tspan, p)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "True solution"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sol = solve(prob, Tsit5())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a sample dataset with some noise."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ts = range(tspan[begin], tspan[end], 200)\n",
    "data = [sol.(ts, idxs=1) sol.(ts, idxs=2)] .* (1 .+ 0.03 .* randn(length(ts), 2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting the sample dataset and the true solution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(sol)\n",
    "scatter!(ts, data, label=[\"u1 data\" \"u2 data\"])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`DiffEqParamEstim.build_loss_objective()` builds a loss function for the ODE problem for the data.\n",
    "\n",
    "We will minimize the mean squared error using `L2Loss()`.\n",
    "\n",
    "Note that\n",
    "- the data should be transposed.\n",
    "- Uses `AutoForwardDiff()` as the automatic differentiation (AD) method since the number of parameters plus states is small (<100). For larger problems, `Optimization.AutoZygote()` is more efficient."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "alg = Tsit5()\n",
    "\n",
    "cost_function = build_loss_objective(\n",
    "    prob, alg,\n",
    "    L2Loss(collect(ts), transpose(data)),\n",
    "    Optimization.AutoForwardDiff(),\n",
    "    maxiters=10000, verbose=false\n",
    ")\n",
    "\n",
    "plot(\n",
    "    cost_function, 0.0, 10.0,\n",
    "    linewidth=3, label=false, yscale=:log10,\n",
    "    xaxis=\"Parameter\", yaxis=\"Cost\", title=\"1-Parameter Cost Function\"\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a dip (minimum) in the cost function at the true parameter value (1.5). We can use an optimizer e.g., `Optimization.jl`, to find the parameter value that minimizes the cost. (1.5 in this case)"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "optprob = Optimization.OptimizationProblem(cost_function, [1.42])\n",
    "optsol = solve(optprob, BFGS())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fitting result:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "newprob = remake(prob, p=optsol.u)\n",
    "newsol = solve(newprob, Tsit5())\n",
    "plot(sol)\n",
    "plot!(newsol)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estimate multiple parameters\n",
    "Let's use the Lotka-Volterra (Fox-rabbit) equations with all 4 parameters free."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function f2(du, u, p, t)\n",
    "    du[1] = dx = p[1] * u[1] - p[2] * u[1] * u[2]\n",
    "    du[2] = dy = -p[3] * u[2] + p[4] * u[1] * u[2]\n",
    "end\n",
    "\n",
    "u0 = [1.0; 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5, 1.0, 3.0, 1.0]  ## True parameters\n",
    "alg = Tsit5()\n",
    "prob = ODEProblem(f2, u0, tspan, p)\n",
    "sol = solve(prob, alg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ts = range(tspan[begin], tspan[end], 200)\n",
    "data = [sol.(ts, idxs=1) sol.(ts, idxs=2)] .* (1 .+ 0.01 .* randn(length(ts), 2))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can find multiple parameters at once using the same steps. True parameters are `[1.5, 1.0, 3.0, 1.0]`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cost_function = build_loss_objective(\n",
    "    prob, alg, L2Loss(collect(ts), transpose(data)),\n",
    "    Optimization.AutoForwardDiff(),\n",
    "    maxiters=10000, verbose=false\n",
    ")\n",
    "optprob = Optimization.OptimizationProblem(cost_function, [1.3, 0.8, 2.8, 1.2])\n",
    "result_bfgs = solve(optprob, LBFGS())"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.6",
   "language": "julia"
  }
 },
 "nbformat": 4
}
